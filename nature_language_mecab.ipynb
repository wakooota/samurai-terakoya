{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsoRYm39jmD5jM5pkYY8ZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wakooota/samurai-terakoya/blob/main/nature_language_mecab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQFXw0tay7wN",
        "outputId": "789588d0-7be1-4d76-c29f-071230ac3b7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-1.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install mecab-python3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidic-lite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVV6j_m6zAi7",
        "outputId": "c6499f2d-d2f6-44d6-a222-344bea449baf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidic-lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658816 sha256=96bbf689f7030b829a64be3fa94dea1a54ad7646025fb6336bef04a1780c68fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite\n",
            "Successfully installed unidic-lite-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "mecab_tagger = MeCab.Tagger()\n",
        "text = \"私は今日、スーパーで沢山のお菓子を買った。\"\n",
        "\n",
        "print(mecab_tagger.parse(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKokN5hbzFmr",
        "outputId": "2e61d74b-1618-4c84-bdec-0c874fdeb8e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私\tワタクシ\tワタクシ\t私-代名詞\t代名詞\t\t\t0\n",
            "は\tワ\tハ\tは\t助詞-係助詞\t\t\t\n",
            "今日\tキョー\tキョウ\t今日\t名詞-普通名詞-副詞可能\t\t\t1\n",
            "、\t\t\t、\t補助記号-読点\t\t\t\n",
            "スーパー\tスーパー\tスーパー\tスーパー-super\t名詞-普通名詞-一般\t\t\t1\n",
            "で\tデ\tデ\tで\t助詞-格助詞\t\t\t\n",
            "沢山\tタクサン\tタクサン\t沢山\t形状詞-一般\t\t\t3\n",
            "の\tノ\tノ\tの\t助詞-格助詞\t\t\t\n",
            "お\tオ\tオ\t御\t接頭辞\t\t\t\n",
            "菓子\tカシ\tカシ\t菓子\t名詞-普通名詞-一般\t\t\t1\n",
            "を\tオ\tヲ\tを\t助詞-格助詞\t\t\t\n",
            "買っ\tカッ\tカウ\t買う\t動詞-一般\t五段-ワア行\t連用形-促音便\t0\n",
            "た\tタ\tタ\tた\t助動詞\t助動詞-タ\t終止形-一般\t\n",
            "。\t\t\t。\t補助記号-句点\t\t\t\n",
            "EOS\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "mecab_tagger = MeCab.Tagger()\n",
        "\n",
        "text = \"私は今日、スーパーで沢山のお菓子を買った。\"\n",
        "node = mecab_tagger.parseToNode(text)\n",
        "print(node)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAfG2GxwzOdx",
        "outputId": "58946db9-9200-477e-95b7-d005cb9419c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Swig Object of type 'MeCab::Node *' at 0x7c2771fc8670>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "mecab_tagger = MeCab.Tagger()\n",
        "\n",
        "text = \"私は今日、スーパーで沢山のお菓子を買った。\"\n",
        "node = mecab_tagger.parseToNode(text)\n",
        "\n",
        "while node:\n",
        "    # 単語、品詞、詳細情報をタブ区切りで表示\n",
        "    print(f'{node.surface}\\t{node.posid}\\t{node.feature}')\n",
        "    # 次の要素を取得\n",
        "    node = node.next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YCiP81y0BV_",
        "outputId": "c0963e72-25fe-4988-8c5a-7bbe32387997"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t0\tBOS/EOS,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*\n",
            "私\t1\t代名詞,*,*,*,*,*,ワタクシ,私-代名詞,私,ワタクシ,私,ワタクシ,和,*,*,*,*,ワタクシ,ワタクシ,ワタクシ,ワタクシ,*,*,0,*,*\n",
            "は\t1\t助詞,係助詞,*,*,*,*,ハ,は,は,ワ,は,ワ,和,*,*,*,*,ハ,ハ,ハ,ハ,*,*,*,\"動詞%F2@0,名詞%F1,形容詞%F2@-1\",*\n",
            "今日\t1\t名詞,普通名詞,副詞可能,*,*,*,キョウ,今日,今日,キョー,今日,キョー,和,*,*,*,*,キョウ,キョウ,キョウ,キョウ,*,*,1,C3,*\n",
            "、\t1\t補助記号,読点,*,*,*,*,,、,、,,、,,記号,*,*,*,*,,,,,*,*,*,*,*\n",
            "スーパー\t1\t名詞,普通名詞,一般,*,*,*,スーパー,スーパー-super,スーパー,スーパー,スーパー,スーパー,外,*,*,*,*,スーパー,スーパー,スーパー,スーパー,*,*,1,C1,*\n",
            "で\t1\t助詞,格助詞,*,*,*,*,デ,で,で,デ,で,デ,和,*,*,*,*,デ,デ,デ,デ,*,*,*,\"動詞%F2@0,名詞%F1\",*\n",
            "沢山\t1\t形状詞,一般,*,*,*,*,タクサン,沢山,沢山,タクサン,沢山,タクサン,漢,タ濁,基本形,*,*,タクサン,タクサン,タクサン,タクサン,*,*,3,C2,*\n",
            "の\t1\t助詞,格助詞,*,*,*,*,ノ,の,の,ノ,の,ノ,和,*,*,*,*,ノ,ノ,ノ,ノ,*,*,*,名詞%F1,*\n",
            "お\t1\t接頭辞,*,*,*,*,*,オ,御,お,オ,お,オ,和,*,*,促添,基本形,オ,オ,オ,オ,*,*,*,P2,*\n",
            "菓子\t1\t名詞,普通名詞,一般,*,*,*,カシ,菓子,菓子,カシ,菓子,カシ,漢,カ濁,基本形,*,*,カシ,カシ,カシ,カシ,*,*,1,C1,*\n",
            "を\t1\t助詞,格助詞,*,*,*,*,ヲ,を,を,オ,を,オ,和,*,*,*,*,ヲ,ヲ,ヲ,ヲ,*,*,*,\"動詞%F2@0,名詞%F1,形容詞%F2@-1\",*\n",
            "買っ\t1\t動詞,一般,*,*,五段-ワア行,連用形-促音便,カウ,買う,買っ,カッ,買う,カウ,和,*,*,*,*,カッ,カウ,カッ,カウ,*,*,0,C4,*\n",
            "た\t1\t助動詞,*,*,*,助動詞-タ,終止形-一般,タ,た,た,タ,た,タ,和,*,*,*,*,タ,タ,タ,タ,*,*,*,\"動詞%F2@1,形容詞%F4@-2\",*\n",
            "。\t1\t補助記号,句点,*,*,*,*,,。,。,,。,,記号,*,*,*,*,,,,,*,*,*,*,*\n",
            "\t0\tBOS/EOS,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "mecab_tagger = MeCab.Tagger()\n",
        "\n",
        "text = '''自然言語処理（しぜんげんごしょり、英語: natural language processing、略称：NLP）は、\n",
        "人間が日常的に使っている自然言語をコンピュータに処理させる一連の技術であり、人工知能と言語学の\n",
        "一分野である。「計算言語学」（computational linguistics）との類似もあるが、自然言語処理は工学的\n",
        "な視点からの言語処理をさすのに対して、計算言語学は言語学的視点を重視する手法をさす事が多い[1]。\n",
        "データベース内の情報を自然言語に変換したり、自然言語の文章をより形式的な（コンピュータが理解し\n",
        "やすい）表現に変換するといった処理が含まれる。応用例としては予測変換、IMEなどの文字変換が挙げら\n",
        "れる。自然言語の理解をコンピュータにさせることは、自然言語理解とされている。自然言語理解と、自\n",
        "然言語処理の差は、意味を扱うか、扱わないかという説もあったが、最近は数理的な言語解析手法（統計\n",
        "や確率など）が広められた為、パーサ（統語解析器）などの精度や速度が一段と上がり、その意味合いは\n",
        "違ってきている。もともと自然言語の意味論的側面を全く無視して達成できることは非常に限られている。\n",
        "このため、自然言語処理には形態素解析と構文解析、文脈解析、意味解析などをSyntaxなど表層的な観点\n",
        "から解析をする学問であるが、自然言語理解は、意味をどのように理解するかという個々人の理解と推論\n",
        "部分が主な研究の課題になってきており、両者の境界は意思や意図が含まれるかどうかになってきている。'''\n",
        "node = mecab_tagger.parseToNode(text)\n",
        "count_dict = {}\n",
        "\n",
        "while node:\n",
        "    word = node.surface\n",
        "    hinshi = node.feature.split(\",\")[0]\n",
        "    if word in count_dict.keys() and hinshi == \"名詞\":\n",
        "        count_dict[word] += 1\n",
        "    elif hinshi == \"名詞\":\n",
        "        count_dict[word] = 1\n",
        "    else:\n",
        "        pass\n",
        "    node = node.next\n",
        "\n",
        "word_counts = sorted(count_dict.items(),  reverse=True)\n",
        "word_counts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh-8LjcH0gnJ",
        "outputId": "148d8eef-67b2-413f-8dd9-2d6f8dbc936f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('類似', 1),\n",
              " ('重視', 1),\n",
              " ('部分', 1),\n",
              " ('達成', 1),\n",
              " ('速度', 1),\n",
              " ('論', 1),\n",
              " ('課題', 1),\n",
              " ('説', 1),\n",
              " ('計算', 2),\n",
              " ('言語', 18),\n",
              " ('解析', 7),\n",
              " ('観点', 1),\n",
              " ('視点', 2),\n",
              " ('表現', 1),\n",
              " ('表層', 1),\n",
              " ('英語', 1),\n",
              " ('自然', 11),\n",
              " ('自', 1),\n",
              " ('統語', 1),\n",
              " ('統計', 1),\n",
              " ('精度', 1),\n",
              " ('確率', 1),\n",
              " ('研究', 1),\n",
              " ('知能', 1),\n",
              " ('略称', 1),\n",
              " ('理解', 7),\n",
              " ('無視', 1),\n",
              " ('為', 1),\n",
              " ('構文', 1),\n",
              " ('最近', 1),\n",
              " ('日常', 1),\n",
              " ('文脈', 1),\n",
              " ('文章', 1),\n",
              " ('文字', 1),\n",
              " ('数理', 1),\n",
              " ('推論', 1),\n",
              " ('技術', 1),\n",
              " ('手法', 2),\n",
              " ('意思', 1),\n",
              " ('意図', 1),\n",
              " ('意味', 5),\n",
              " ('情報', 1),\n",
              " ('応用', 1),\n",
              " ('形態', 1),\n",
              " ('形式', 1),\n",
              " ('差', 1),\n",
              " ('工学', 1),\n",
              " ('学問', 1),\n",
              " ('変換', 4),\n",
              " ('境界', 1),\n",
              " ('分野', 1),\n",
              " ('処理', 7),\n",
              " ('側面', 1),\n",
              " ('個々', 1),\n",
              " ('例', 1),\n",
              " ('人間', 1),\n",
              " ('人工', 1),\n",
              " ('事', 1),\n",
              " ('予測', 1),\n",
              " ('両者', 1),\n",
              " ('一連', 1),\n",
              " ('一段', 1),\n",
              " ('一', 1),\n",
              " ('パーサ', 1),\n",
              " ('データベース', 1),\n",
              " ('コンピュータ', 3),\n",
              " ('ため', 1),\n",
              " ('こと', 2),\n",
              " ('げん', 1),\n",
              " ('processing', 1),\n",
              " ('natural', 1),\n",
              " ('linguistics', 1),\n",
              " ('language', 1),\n",
              " ('computational', 1),\n",
              " ('Syntax', 1),\n",
              " ('NLP', 1),\n",
              " ('IME', 1),\n",
              " ('1', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#名詞のみのリストを取得\n",
        "\n",
        "import re\n",
        "import MeCab\n",
        "\n",
        "mecab_tagger = MeCab.Tagger()\n",
        "\n",
        "text = '''自然言語処理（しぜんげんごしょり、英語: natural language processing、略称：NLP）は、\n",
        "人間が日常的に使っている自然言語をコンピュータに処理させる一連の技術であり、人工知能と言語学の\n",
        "一分野である。「計算言語学」（computational linguistics）との類似もあるが、自然言語処理は工学的\n",
        "な視点からの言語処理をさすのに対して、計算言語学は言語学的視点を重視する手法をさす事が多い[1]。\n",
        "データベース内の情報を自然言語に変換したり、自然言語の文章をより形式的な（コンピュータが理解し\n",
        "やすい）表現に変換するといった処理が含まれる。応用例としては予測変換、IMEなどの文字変換が挙げら\n",
        "れる。自然言語の理解をコンピュータにさせることは、自然言語理解とされている。自然言語理解と、自\n",
        "然言語処理の差は、意味を扱うか、扱わないかという説もあったが、最近は数理的な言語解析手法（統計\n",
        "や確率など）が広められた為、パーサ（統語解析器）などの精度や速度が一段と上がり、その意味合いは\n",
        "違ってきている。もともと自然言語の意味論的側面を全く無視して達成できることは非常に限られている。\n",
        "このため、自然言語処理には形態素解析と構文解析、文脈解析、意味解析などをSyntaxなど表層的な観点\n",
        "から解析をする学問であるが、自然言語理解は、意味をどのように理解するかという個々人の理解と推論\n",
        "部分が主な研究の課題になってきており、両者の境界は意思や意図が含まれるかどうかになってきている。'''\n",
        "node = mecab_tagger.parseToNode(text)\n",
        "vocab_list = []\n",
        "\n",
        "while node:\n",
        "    word = node.surface\n",
        "    hinshi = node.feature.split(\",\")[0]\n",
        "    if hinshi == \"名詞\":\n",
        "        #isnumeric()は文字列が数字を表すかどうかを判断するメソッド\n",
        "        #^[\\u3040-\\u309F]+$はひらがなのみの場合を判断するメソッド\n",
        "        if (not word.isnumeric()) and (not re.match(r'^[\\u3040-\\u309F]+$', word)):\n",
        "            # 名詞が数値と平仮名のみの場合は除き、それ以外の名詞を保存\n",
        "            vocab_list.append(word)\n",
        "    else:\n",
        "        pass\n",
        "    node = node.next\n",
        "\n",
        "print(vocab_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx92xmQp2sHs",
        "outputId": "c673133e-d6fe-41b0-fce9-72facd9e6ed7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['自然', '言語', '処理', '英語', 'natural', 'language', 'processing', '略称', 'NLP', '人間', '日常', '自然', '言語', 'コンピュータ', '処理', '一連', '技術', '人工', '知能', '言語', '分野', '計算', '言語', 'computational', 'linguistics', '類似', '自然', '言語', '処理', '工学', '視点', '言語', '処理', '計算', '言語', '言語', '視点', '重視', '手法', '事', 'データベース', '情報', '自然', '言語', '変換', '自然', '言語', '文章', '形式', 'コンピュータ', '理解', '表現', '変換', '処理', '応用', '例', '予測', '変換', 'IME', '文字', '変換', '自然', '言語', '理解', 'コンピュータ', '自然', '言語', '理解', '自然', '言語', '理解', '自', '言語', '処理', '差', '意味', '説', '最近', '数理', '言語', '解析', '手法', '統計', '確率', '為', 'パーサ', '統語', '解析', '精度', '速度', '一段', '意味', '自然', '言語', '意味', '論', '側面', '無視', '達成', '自然', '言語', '処理', '形態', '解析', '構文', '解析', '文脈', '解析', '意味', '解析', 'Syntax', '表層', '観点', '解析', '学問', '自然', '言語', '理解', '意味', '理解', '個々', '理解', '推論', '部分', '研究', '課題', '両者', '境界', '意思', '意図']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE-9lwbM27FV",
        "outputId": "8109e1fd-3c9b-4831-9e3a-b0e8f23cf54e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post10.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post10-py3-none-any.whl size=2959 sha256=d49889815c817bfd9b82f0397acaef6127f118b32647fd8ef202390f56e0b207\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/f6/92/0173054cc528db7ffe7b0c7652a96c3102aab156a6da960387\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# tfidfモデルの作成と学習\n",
        "tfidf_model = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', norm=None)\n",
        "tfidf_model.fit(vocab_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "us4zrnHu3Bxw",
        "outputId": "a8d0692c-d6d1-4591-e9b4-47757b6bb7ff"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(norm=None, token_pattern='(?u)\\\\b\\\\w+\\\\b')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(norm=None, token_pattern=&#x27;(?u)\\\\b\\\\w+\\\\b&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(norm=None, token_pattern=&#x27;(?u)\\\\b\\\\w+\\\\b&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 対象のテキストをtf-idf値に変換\n",
        "vocab_text = \" \".join(vocab_list)\n",
        "tfidf_vec = tfidf_model.transform([vocab_text]).toarray()[0]\n",
        "# 単語: tf-idf値となるdictに変換\n",
        "tfidf_dict = dict(zip(tfidf_model.get_feature_names(), tfidf_vec))\n",
        "# tf-idf値が正のみの単語を残す\n",
        "tfidf_dict = {word: num_val for word, num_val in tfidf_dict.items() if num_val > 0}\n",
        "\n",
        "tfidf_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "VETCPw3B3LTm",
        "outputId": "99405088-2fa8-4c60-fae5-9708cb5800d8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d09ac86a947d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtfidf_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 単語: tf-idf値となるdictに変換\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtfidf_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# tf-idf値が正のみの単語を残す\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtfidf_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnum_val\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtfidf_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_val\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hukMRunD3Sf2",
        "outputId": "f06c8057-65cd-431f-a820-d438672d3266"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.2)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -y install fonts-ipafont-gothic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Osa-A3Ap3W_8",
        "outputId": "ded81ed8-94d3-4ebf-cedc-3a41019a0889"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-ipafont-gothic is already the newest version (00303-21ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "font_path = \"/usr/share/fonts/truetype/fonts-japanese-gothic.ttf\"\n",
        "wc = WordCloud(background_color=\"white\",width=900, height=500, font_path=font_path).generate_from_frequencies(tfidf_dict)\n",
        "plt.figure(figsize=(18,10))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "tMihOzyh3axO",
        "outputId": "3a553674-b8a5-4f4c-c329-3f971e9d2a1d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-374d0b1cb9e6>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfont_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/usr/share/fonts/truetype/fonts-japanese-gothic.ttf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfont_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tfidf_dict' is not defined"
          ]
        }
      ]
    }
  ]
}